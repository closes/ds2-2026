{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Clustering - Tuto 1\n",
    "\n",
    "In this tuto we play with a realistic 1D dataset: Vertical mean temperature from Argo data\n",
    "\n",
    "We use clustering with GMM to analyse the PDF of the dataset.\n",
    "\n",
    "The goal is to understand better how GMM works, and its limitations.\n",
    "\n",
    "The optimal number of clusters is determined with BIC and independant samples issues are demonstrated\n",
    "\n",
    "(c) G. Maze\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "*First, let's make sure the Python env is correct to run this notebook*:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os, sys, urllib, tempfile\n",
    "with tempfile.TemporaryDirectory() as local:\n",
    "    sys.path.append(local)\n",
    "    urllib.request.urlretrieve(\"https://raw.githubusercontent.com/obidam/ds2-2026/main/utils.py\", os.path.join(local, \"utils.py\"))\n",
    "    from utils import check_up_env\n",
    "    check_up_env(with_tuto=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with tempfile.TemporaryDirectory() as local:\n",
    "    sys.path.append(local)\n",
    "    urllib.request.urlretrieve(\"https://raw.githubusercontent.com/obidam/ds2-2026/main/practice/exploratory_statistics/tuto_tools.py\", os.path.join(local, \"tuto_tools.py\"))\n",
    "    from tuto_tools import create_map, gaussian, plot_normal"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Libraries import section\n",
    "import os, sys\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy import signal\n",
    "from intake import open_catalog\n",
    "\n",
    "from sklearn import preprocessing\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "\n",
    "import matplotlib as mpl\n",
    "# matplotlib.use('agg')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "sns.set(context=\"notebook\", style=\"whitegrid\", palette=\"deep\", color_codes=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "From Google cloud storage [see specific tuto here](https://github.com/obidam/ds2-2023/blob/main/practice/environment/02-Access_to_data_in_the_cloud.ipynb).\n",
    "\n",
    "We work with a 1-dimensional dataset: local 0-2000m vertical mean measurements (eg: temperature, salinity), from Argo floats."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "catalog_url = 'https://raw.githubusercontent.com/obidam/ds2-2026/main/ds2_data_catalog.yml'\n",
    "cat = open_catalog(catalog_url)\n",
    "ds = cat['argo_global_vertical_mean'].read_chunked()\n",
    "print('This dataset holds: %.3f GB' % (ds.nbytes / 1e9))\n",
    "print(ds)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats for 1D data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "X, Xlabel = ds['TEMP'], \"VERTICAL MEAN TEMP\"\n",
    "# X = X[np.where(X>3)]\n",
    "print(X)\n",
    "print(Xlabel)\n",
    "print(X.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, proj, ax = create_map()\n",
    "plt.scatter(X.LONGITUDE, X.LATITUDE, 1, X, cmap=mpl.colormaps.get_cmap('gist_ncar'), vmin=0, vmax=16)\n",
    "plt.colorbar()\n",
    "plt.title(Xlabel)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=plt.figaspect(0.5), dpi=120)\n",
    "axes.set_title('Histogram')\n",
    "sns.histplot(X, ax=axes)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process data\n",
    "\n",
    "Normalisation step"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%time\n",
    "# From xarray, X has shape: (N_PROF,)\n",
    "# For scikit-learn we need X with shape: [N_PROF,1]\n",
    "X0 = X.values[np.newaxis].T\n",
    "print(\"Data shape [n_samples, n_features]:\", X0.shape) # shape [n_samples, n_features=1]\n",
    "\n",
    "# Fit the scaler object:\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler = scaler.fit(X0)\n",
    "\n",
    "# The mean and std profiles are in the scaler object properties:\n",
    "X_ave = scaler.mean_\n",
    "X_std = scaler.scale_\n",
    "print(\"Data mean, std:\", X_ave, X_std)\n",
    "\n",
    "# Normalize data:\n",
    "Xn = scaler.transform(X0)       \n",
    "\n",
    "# Here, we only center data:\n",
    "Xc = preprocessing.StandardScaler(with_std=False).fit(X0).transform(X0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=plt.figaspect(0.5), dpi=120)\n",
    "axes.set_title('PDF and Gaussian fit to normalised data')\n",
    "ax = sns.histplot(Xn, stat='density', kde=False)\n",
    "plot_normal(np.mean(Xn), np.std(Xn), color=ax.patches[-1].get_facecolor())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering in 1D\n",
    "\n",
    "Clearly from the figure above, one can see that the dataset is not Gaussian and exhibits several modes. In other words, data samples agregated into several clusters.\n",
    "\n",
    "Let's identify them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a GMM to identify modes (the \"clusters\") in the distribution"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "\n",
    "# Clustering with GMM:\n",
    "classifier = GMM(n_components=4)\n",
    "classifier = classifier.fit(Xn)\n",
    "print(classifier)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Predict class labels:\n",
    "labels = classifier.predict(Xn)\n",
    "print(labels)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# See more about clusters:\n",
    "n_clusters = np.unique(labels).shape[0]\n",
    "for k in range(n_clusters):\n",
    "    print(\"Cluster %i mean(std), weight: %0.2f (%0.2f), %0.2f\"%(k, \n",
    "                                                                classifier.means_[k,0], \n",
    "                                                                classifier.covariances_[k,0],\n",
    "                                                                classifier.weights_[k]))\n",
    "print(np.sum(classifier.weights_)) # Must be 1."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compute the GMM dataset pdf:\n",
    "x = np.linspace(-3,3,200)\n",
    "gmm_pdf = np.zeros(x.shape)\n",
    "for k in range(n_clusters):\n",
    "    gmm_pdf += classifier.weights_[k]*gaussian(x, \n",
    "                                               classifier.means_[k,0], \n",
    "                                               classifier.covariances_[k,0])    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "colors = sns.husl_palette(n_clusters)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,5), dpi=90)\n",
    "ax = sns.histplot(Xn, stat='density', kde=False, ax=ax)\n",
    "ax.set_title('Histogram of normalised data with GMM clustering results (%i clusters)' % n_clusters)\n",
    "plt.plot(x,gmm_pdf,'k', linewidth=2)\n",
    "for k, col in zip(range(n_clusters),colors):\n",
    "    plt.plot(x,classifier.weights_[k]*\n",
    "             gaussian(x, classifier.means_[k,0], classifier.covariances_[k,0]),\\\n",
    "             color=col, linewidth=2, \n",
    "             label=\"$\\lambda_%i=%0.0f$%%: $\\mu_%i$=%0.2f ($\\sigma^2_%i=%0.2f$)\"%(k, \n",
    "                                                    classifier.weights_[k]*100, k, \n",
    "                                                    classifier.means_[k,0], k, \n",
    "                                                    classifier.covariances_[k,0]))\n",
    "plt.legend()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to select the nb of cluster ?\n",
    "\n",
    "This is the most problematic question in clustering.\n",
    "\n",
    "It is a rather difficult problem to determine automatically the most appropriate number of components. There exist different methods that are mostly based on estimating the most probable K, or minimizing a given metric such as the mixture entropy or misfit with the observed PDF (Fraley et al., 1998). A popular method is the Bayesian Information Criterion (BIC, Schwarz, 1978). The BIC is an empirical approach of the model probability computed as:\n",
    "\n",
    "$$BIC(K) = -2\\,\\mathcal{L}(K) + N_f(K)\\,\\log(n) \\label{eq:bic}$$\n",
    "\n",
    "where $\\mathcal{L}(K)$ is the log likelihood of the trained model with $K$ classes, $N_f(K)=K-1+K\\,D+K\\,D\\,(D+1)/2$ is the number of independent parameters to be estimated (the sum of the component weights, Gaussian means and covariance matrix elements in the D-dimensional data space) and $n$ is the number of profiles used to train the model.\n",
    "\n",
    "The BIC is empirical because the first r.h.s. term decreases as the number of classes K increases while the second r.h.s. term is a penalty term that increases with K and thus prevents model overfitting the data. The sum of the two terms is expected to exhibit a minimum for the most appropriate $K$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "kmax = 20\n",
    "BIC = np.zeros((kmax))\n",
    "BICf = np.zeros((kmax))\n",
    "\n",
    "print(Xn.shape[0])\n",
    "n = Xn.shape[0] # Nb of samples\n",
    "n = 900\n",
    "n = 2196\n",
    "\n",
    "for k in range(kmax):\n",
    "    print(k, kmax)\n",
    "    this_gmm = GMM(n_components=k+1).fit(Xn)\n",
    "    BIC[k] = this_gmm.bic(Xn)\n",
    "    D = 1 # Nb of dimension\n",
    "    Nf = (k+1)-1 + (k+1)*D + (k+1)*D*(D+1)/2 # Nb of independant parameters to estimate\n",
    "#     print Nf, this_gmm._n_parameters()\n",
    "    BICf[k] = -2*n*this_gmm.score(Xn) + Nf*np.log(n)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,5), dpi=90)\n",
    "plt.plot(np.arange(kmax)+1,(BIC-np.mean(BIC))/np.std(BIC),label='Raw BIC')\n",
    "plt.plot(np.arange(kmax)+1,(BICf-np.mean(BICf))/np.std(BICf), label='BIC using only samples of independant observations')\n",
    "plt.ylabel('Normalized BIC')\n",
    "plt.xticks(np.arange(kmax)+1)\n",
    "plt.legend()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Since we have more observations than the maximum independant sample size, \n",
    "# we can run several BIC computations\n",
    "kmax = 20\n",
    "Nrun = 30\n",
    "BIC = np.zeros((kmax,Nrun))\n",
    "BICf = np.zeros((kmax,Nrun))\n",
    "\n",
    "print(Xn.shape[0])\n",
    "n = Xn.shape[0] # Nb of samples\n",
    "n = 2196 # Nb of independant samples\n",
    "\n",
    "for run in range(Nrun):\n",
    "    print(run, Nrun)\n",
    "    for k in range(kmax):\n",
    "        ii = np.random.choice(range(X.shape[0]), n, replace=False)\n",
    "        this_gmm = GMM(n_components=k+1).fit(Xn[ii])\n",
    "        BIC[k,run] = this_gmm.bic(Xn[ii])\n",
    "        D = 1 # Nb of dimension\n",
    "        Nf = (k+1)-1 + (k+1)*D + (k+1)*D*(D+1)/2 # Nb of independant parameters to estimate\n",
    "    #     print Nf, this_gmm._n_parameters()\n",
    "        BICf[k,run] = -2*n*this_gmm.score(Xn[ii]) + Nf*np.log(n)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,5), dpi=90)\n",
    "BICfmean = np.mean(BICf,axis=1)\n",
    "BICfstd = np.std(BICf,axis=1)\n",
    "normBICfmean = (BICfmean-np.mean(BICfmean))/np.std(BICfmean)\n",
    "plt.plot(np.arange(kmax)+1,BICfmean, \n",
    "         label='BIC using independant observations')\n",
    "plt.plot(np.arange(kmax)+1,BICfmean+BICfstd,color=[0.7]*3,linewidth=0.5)\n",
    "plt.plot(np.arange(kmax)+1,BICfmean-BICfstd,color=[0.7]*3,linewidth=0.5)\n",
    "plt.ylabel('BIC')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.xticks(np.arange(kmax)+1)\n",
    "plt.legend()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Change the nb of clusters in the GMM.\n",
    "\n",
    "What happens when K=2 ?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "\n",
    "# Clustering with GMM:\n",
    "classifier = GMM(n_components=2)\n",
    "classifier = classifier.fit(Xn)\n",
    "print(classifier)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds2-coiled-2025-binder",
   "language": "python",
   "name": "ds2-coiled-2025-binder"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
